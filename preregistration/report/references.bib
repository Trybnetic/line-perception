@article{muller1889optische,
	title={Optische urteilst{\"a}uschungen},
	author={M{\"u}ller-Lyer, Franz C},
	journal={Archiv f{\"u}r Anatomie und Physiologie, Physiologische Abteilung},
	volume={2},
	number={9},
	pages={263--270},
	year={1889}
}

@article{greistbousquet1981,
	author = {Suzanne Greist-Bousquet and Harvey R Schiffman},
	title ={The Poggendorff Illusion: An Illusion of Linear Extent?},
	journal = {Perception},
	volume = {10},
	number = {2},
	pages = {155-164},
	year = {1981},
	doi = {10.1068/p100155}
}

@misc{mcdonnell2012psiturk,
	title={psiTurk (Version 1. 02)[Software]},
	author={McDonnell, JV and Martin, JB and Markant, DB and Coenen, A and Rich, AS and Gureckis, TM},
	year={2012},
	url={ https://github.com/NYUCCL/psiTurk}
}


@Article{Gureckis2016,
	author="Gureckis, Todd M.
	and Martin, Jay 
	and McDonnell, John
	and Rich, Alexander S.
	and Markant, Doug
	and Coenen, Anna
	and Halpern, David
	and Hamrick, Jessica B.
	and Chan, Patricia",
	title="psiTurk: An open-source framework for conducting replicable behavioral experiments online",
	journal="Behavior Research Methods",
	year="2016",
	month="Sep",
	day="01",
	volume="48",
	number="3",
	pages="829--842",
	abstract="Online data collection has begun to revolutionize the behavioral sciences. However, conducting carefully controlled behavioral experiments online introduces a number of new of technical and scientific challenges. The project described in this paper, psiTurk, is an open-source platform which helps researchers develop experiment designs which can be conducted over the Internet. The tool primarily interfaces with Amazon's Mechanical Turk, a popular crowd-sourcing labor market. This paper describes the basic architecture of the system and introduces new users to the overall goals. psiTurk aims to reduce the technical hurdles for researchers developing online experiments while improving the transparency and collaborative nature of the behavioral sciences.",
	issn="1554-3528",
	doi="10.3758/s13428-015-0642-8",
	url="https://doi.org/10.3758/s13428-015-0642-8"
}

@Article{Mason2012,
	author="Mason, Winter
	and Suri, Siddharth",
	title="Conducting behavioral research on Amazon's Mechanical Turk",
	journal="Behavior Research Methods",
	year="2012",
	month="Mar",
	day="01",
	volume="44",
	number="1",
	pages="1--23",
	abstract="Amazon's Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.",
	issn="1554-3528",
	doi="10.3758/s13428-011-0124-6",
	url="https://doi.org/10.3758/s13428-011-0124-6"
}

@article{paolacci2010,
	title="Running experiments on amazon mechanical turk",
	author="Paolacci, Gabriele and Chandler, Jesse and Ipeirotis, Panagiotis G",
	year="2010",
	month="Jun",
	day="24",
	journal="Judgment and Decision Making",
	volume="5",
	number="5",
	pages="411--419"
}

@article {aac4716,
	author = {{Open Science Collaboration}},
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	number = {6251},
	year = {2015},
	doi = {10.1126/science.aac4716},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/349/6251/aac4716},
	eprint = {http://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	journal = {Science}
}

@article{buhrmeister2011,
	author = {Michael Buhrmester and Tracy Kwang and Samuel D. Gosling},
	title ={Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?},
	journal = {Perspectives on Psychological Science},
	volume = {6},
	number = {1},
	pages = {3-5},
	year = {2011},
	doi = {10.1177/1745691610393980},
	note ={PMID: 26162106},
	
	URL = { 
	https://doi.org/10.1177/1745691610393980
	
	},
	eprint = { 
	https://doi.org/10.1177/1745691610393980
	
	}
	,
	abstract = { Amazon’s Mechanical Turk (MTurk) is a relatively new website that contains the major elements required to conduct research: an integrated participant compensation system; a large participant pool; and a streamlined process of study design, participant recruitment, and data collection. In this article, we describe and evaluate the potential contributions of MTurk to psychology and other social sciences. Findings indicate that (a) MTurk participants are slightly more demographically diverse than are standard Internet samples and are significantly more diverse than typical American college samples; (b) participation is affected by compensation rate and task length, but participants can still be recruited rapidly and inexpensively; (c) realistic compensation rates do not affect data quality; and (d) the data obtained are at least as reliable as those obtained via traditional methods. Overall, MTurk can be used to obtain high-quality data inexpensively and rapidly. }
}

@article{gosling2010, 
	title={Wired but not WEIRD: The promise of the Internet in reaching more diverse samples}, 
	volume={33}, 
	DOI={10.1017/S0140525X10000300}, 
	number={2-3}, 
	journal={Behavioral and Brain Sciences}, 
	publisher={Cambridge University Press}, 
	author={Gosling, Samuel D. and Sandy, Carson J. and John, Oliver P. and Potter, Jeff}, 
	year={2010}, 
	pages={94–95}}

@book{gosling2010advanced,
	title={Advanced methods for conducting online behavioral research.},
	author={Gosling, Samuel D and Johnson, John A},
	year={2010},
	publisher={American Psychological Association}
}

@Manual{python3,
	title = {Python 3},
	author = {{Python Foundation}},
	organization = {Python Foundation},
	year = {2017},
	url = {http://www.python.org},
}

@Manual{ronacher2010flask,
	title={Flask (a Python microframework)},
	author={Ronacher, Armin and Brandl, G and Zapletal, A and Afshar, A and Edgemon, C and Grindstaff, C and Grebs, C and Neuh{\"a}user, D and Xicluna, F and Brandl, G and others},
	url={http://flask.pocoo.org},
	year = {2017},
}

@article{wood2011fast,
	title={Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	author={Simon N Wood},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={73},
	number={1},
	pages={3--36},
	year={2011},
	publisher={Wiley Online Library}
}

@book{wood2006generalized,
	title={Generalized additive models: an introduction with R},
	author={Simon N Wood},
	year={2006},
	publisher={CRC press}
}

@Article{deLeeuw2016,
	author="de Leeuw, Joshua R.
	and Motz, Benjamin A.",
	title="Psychophysics in a Web browser? Comparing response times collected with JavaScript and Psychophysics Toolbox in a visual search task",
	journal="Behavior Research Methods",
	year="2016",
	month="Mar",
	day="01",
	volume="48",
	number="1",
	pages="1--12",
	abstract="Behavioral researchers are increasingly using Web-based software such as JavaScript to conduct response time experiments. Although there has been some research on the accuracy and reliability of response time measurements collected using JavaScript, it remains unclear how well this method performs relative to standard laboratory software in psychologically relevant experimental manipulations. Here we present results from a visual search experiment in which we measured response time distributions with both Psychophysics Toolbox (PTB) and JavaScript. We developed a methodology that allowed us to simultaneously run the visual search experiment with both systems, interleaving trials between two independent computers, thus minimizing the effects of factors other than the experimental software. The response times measured by JavaScript were approximately 25 ms longer than those measured by PTB. However, we found no reliable difference in the variability of the distributions related to the software, and both software packages were equally sensitive to changes in the response times as a result of the experimental manipulations. We concluded that JavaScript is a suitable tool for measuring response times in behavioral research.",
	issn="1554-3528",
	doi="10.3758/s13428-015-0567-2",
	url="https://doi.org/10.3758/s13428-015-0567-2"
}

@Article{Hilbig2016,
	author="Hilbig, Benjamin E.",
	title="Reaction time effects in lab- versus Web-based research: Experimental evidence",
	journal="Behavior Research Methods",
	year="2016",
	month="Dec",
	day="01",
	volume="48",
	number="4",
	pages="1718--1724",
	abstract="Although Web-based research is now commonplace, it continues to spur skepticism from reviewers and editors, especially whenever reaction times are of primary interest. Such persistent preconceptions are based on arguments referring to increased variation, the limits of certain software and technologies, and a noteworthy lack of comparisons (between Web and lab) in fully randomized experiments. To provide a critical test, participants were randomly assigned to complete a lexical decision task either (a) in the lab using standard experimental software (E-Prime), (b) in the lab using a browser-based version (written in HTML and JavaScript), or (c) via the Web using the same browser-based version. The classical word frequency effect was typical in size and corresponded to a very large effect in all three conditions. There was no indication that the Web- or browser-based data collection was in any way inferior. In fact, if anything, a larger effect was obtained in the browser-based conditions than in the condition relying on standard experimental software. No differences between Web and lab (within the browser-based conditions) could be observed, thus disconfirming any substantial influence of increased technical or situational variation. In summary, the present experiment contradicts the still common preconception that reaction time effects of only a few hundred milliseconds cannot be detected in Web experiments.",
	issn="1554-3528",
	doi="10.3758/s13428-015-0678-9",
	url="https://doi.org/10.3758/s13428-015-0678-9"
}


